{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDDT-SmAa9kP"
      },
      "source": [
        "#Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rMiWopwVS_T"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "device = torch.device(\"cuda\")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.patches import Rectangle\n",
        "img_dir = \"/content/drive/MyDrive/ColabNotebooks/RelaxedOriginalCode/MyGraphs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRatlTd2lt6u"
      },
      "outputs": [],
      "source": [
        "#Function to download the data and sort it into batches which can be used to train our data\n",
        "def import_data(data_name):\n",
        "  tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
        "  if data_name == \"cifar\":\n",
        "    train_set = torchvision.datasets.CIFAR10(\"/content/gdrive/ColabNotebooks/Relaxed Original Code\", train=True, download=True, transform=tensor_transform)\n",
        "    test_set = torchvision.datasets.CIFAR10(\"/content/gdrive/ColabNotebooks/Relaxed Original Code\", train=False, download=True, transform=tensor_transform)\n",
        "  elif data_name == \"fashion\":\n",
        "    train_set = torchvision.datasets.FashionMNIST(\"/content/gdrive/ColabNotebooks/Relaxed Original Code\", train=True, download=True, transform=tensor_transform)\n",
        "    test_set = torchvision.datasets.FashionMNIST(\"/content/gdrive/ColabNotebooks/Relaxed Original Code\", train=False, download=True, transform=tensor_transform)\n",
        "  elif data_name == \"mnist\":\n",
        "    train_set = torchvision.datasets.MNIST(\"/content/gdrive/ColabNotebooks/Relaxed Original Code\", train=True, download=True, transform=tensor_transform)\n",
        "    test_set = torchvision.datasets.MNIST(\"/content/gdrive/ColabNotebooks/Relaxed Original Code\", train=False, download=True, transform=tensor_transform)\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, drop_last = True) #Shuffles the data into batches so we can train on smaller sets at a time\n",
        "  test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True, drop_last = True)\n",
        "  train_set = list(iter(train_dataloader))\n",
        "  test_set = list(iter(test_dataloader))\n",
        "  display = test_set.copy()\n",
        "  for i,(img, label) in enumerate(train_set): #Reshape the images to be a 1 dimensional array\n",
        "        train_set[i] = (img.reshape(len(img),-1),label)\n",
        "  for i,(img, label) in enumerate(test_set):\n",
        "        test_set[i] = (img.reshape(len(img),-1) ,label)\n",
        "  return train_set, test_set, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J8f-cvaqtgu"
      },
      "outputs": [],
      "source": [
        "#Onehots an array so each class is its own individual index\n",
        "def onehot(label, class_size):\n",
        "  oh_label = torch.zeros([len(label),class_size])\n",
        "  for i in range(len(label)):\n",
        "    oh_label[i,label[i]] = 1\n",
        "  return oh_label.float().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "WivQW0llsHvu"
      },
      "outputs": [],
      "source": [
        "#Activation Functions to use for our model\n",
        "def tanh(matrix):\n",
        "  return torch.tanh(matrix)\n",
        "\n",
        "def d_tanh(matrix):\n",
        "  return 1.0 - (torch.tanh(matrix)**2)\n",
        "\n",
        "def relu(matrix):\n",
        "  f = torch.nn.ReLU()\n",
        "  return f(matrix)\n",
        "\n",
        "def d_relu(matrix):\n",
        "  rel = relu(matrix)\n",
        "  rel[rel>0] = 1\n",
        "  return rel\n",
        "\n",
        "def sigm(matrix):\n",
        "  sig = nn.Sigmoid()\n",
        "  return sig(matrix)\n",
        "\n",
        "def d_sigm(matrix):\n",
        "  sig_m = sigm(matrix)\n",
        "  return sig_m * (1-sig_m)\n",
        "\n",
        "#Turns the matrix into a torch tensor\n",
        "def set_tensor(matrix):\n",
        "  return matrix.float().to(device)\n",
        "\n",
        "#Takes the onehotted predicted and true labels and compares them, if the max value of the \n",
        "def accuracy(prediction, label):\n",
        "  batch_size,l = prediction.shape\n",
        "  total = 0\n",
        "  for i in range(batch_size):\n",
        "    if torch.argmax(prediction[i,:]) == torch.argmax(label[i,:]):\n",
        "      total +=1\n",
        "  return total/ batch_size\n",
        "\n",
        "#Produces a graph of test_data accuracy over a number of epochs and saves it to the directory\n",
        "def depict(results, func, dset, v_name, o_res):\n",
        "    accuracy_df = pd.DataFrame(results)\n",
        "    accuracy_df = pd.concat((accuracy_df, o_res), axis=1)\n",
        "    sns.set(rc={\"figure.figsize\":(5,3)})\n",
        "    accuracy_df.plot()\n",
        "    plt.title(f\"{v_name} Accuracy\")\n",
        "    plt.xlabel(\"Accuracy\")\n",
        "    plt.ylabel(\"Epochs\")\n",
        "    plt.legend([\"Relaxed\", \"Original\"])\n",
        "    plt.savefig(f\"{img_dir}/{v_name}_{dset}_{func}_Accuracy_Graph.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia_TFvQJckRc"
      },
      "outputs": [],
      "source": [
        "class Pred_Layer():\n",
        "  def __init__(self, input_size, output_size, batch_size=64, learning_rate=0.0005, activation_function=tanh, act_func_deriv=d_tanh, use_back_weights = True, use_non_linear = False):\n",
        "    #Prepare all layer specific variables\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.activation_function = activation_function\n",
        "    self.act_func_deriv = act_func_deriv\n",
        "    self.bias = torch.zeros([batch_size, self.output_size]).to(device)\n",
        "    self.use_back_weights = use_back_weights\n",
        "    self.use_non_linear = use_non_linear\n",
        "    #Prepare the random weights\n",
        "    self.weights = torch.empty([self.input_size,self.output_size]).normal_(mean=0.0,std=0.05).to(device) #Forward fed weight matrix\n",
        "    self.back_weights = torch.empty([self.output_size, self.input_size]).normal_(mean=0.0,std=0.05).to(device) #Backward fed weight matrix\n",
        "\n",
        "  def forward(self, inputs): #Calculates the prediction for the next layer\n",
        "    self.inp = inputs.clone()\n",
        "    #print(\"Input: \", self.inp.size(), \"  Weight:\", self.weights.size())\n",
        "    self.activation = torch.matmul(self.inp,self.weights)\n",
        "    #print(self.activation.size())\n",
        "    return self.activation_function(self.activation) + self.bias\n",
        "  \n",
        "  def backward(self, errors): #Passes errors from layer above back into current layer using weights\n",
        "    activation_divs = self.act_func_deriv(self.activation)\n",
        "    if self.use_back_weights:\n",
        "      if self.use_non_linear:\n",
        "        return torch.matmul(errors*activation_divs, self.back_weights)\n",
        "      else:\n",
        "        return torch.matmul(errors, self.back_weights)\n",
        "    else:\n",
        "      if self.use_non_linear:\n",
        "        return torch.matmul(errors*activation_divs, self.weights.T)\n",
        "      else:\n",
        "        return torch.matmul(errors, self.weights.T)\n",
        "\n",
        "  def update_weights(self, errors): #Updates the weights\n",
        "    activation_divs = self.act_func_deriv(self.activation)\n",
        "    DW_back = torch.matmul((errors*activation_divs).T,self.inp) #change in backward weights\n",
        "    DW_forward = torch.matmul(self.inp.T, errors*activation_divs) #change in forward weights\n",
        "    self.weights += self.learning_rate * torch.clamp(DW_forward*2,-50, 50) #clamped so that it doesn't grow too far\n",
        "    self.back_weights += self.learning_rate * torch.clamp(DW_back*2,-50, 50)\n",
        "    self.bias += self.learning_rate * torch.clamp(errors,-50,50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {
        "id": "K3raL2ogU6g3"
      },
      "outputs": [],
      "source": [
        "class Relaxed_Predictive_Net():\n",
        "  def __init__(self, layers, fn, dataset, n_steps=500, inf_learning_rate=0.01, weight_learning_rate=0.0005, fully_connected = True):\n",
        "    self.layers = layers\n",
        "    self.n_steps = n_steps\n",
        "    self.inf_learning_rate = inf_learning_rate\n",
        "    self.weight_learning_rate = weight_learning_rate\n",
        "    self.fully_connected = fully_connected\n",
        "    self.l_num = len(self.layers)\n",
        "    self.guess = [[] for i in range(self.l_num+1)]\n",
        "    self.pred_e = [[] for i in range(self.l_num+1)]\n",
        "    self.down_error = [[] for i in range(self.l_num+1)]\n",
        "    self.mus = [[] for i in range(self.l_num+1)]\n",
        "    self.fn = fn\n",
        "    self.dataset = dataset\n",
        "    self.error_weights = []\n",
        "    for i,l in enumerate(self.layers): #Generate the full connection matrix, starting with an index matrix for the 1 to 1 connections then randomly changing all values\n",
        "      error_weight = ((0.0 * set_tensor(torch.eye(l.input_size))) + set_tensor(torch.empty([l.input_size, l.input_size]).normal_(mean=0.0, std=0.05))).to(device)\n",
        "      error_weight = torch.abs(error_weight)\n",
        "      self.error_weights.append(error_weight)\n",
        "    self.error_weights.append(set_tensor(torch.eye(self.layers[-1].output_size)))\n",
        "\n",
        "  def forward_pass(self, x): #Gets a prediction for given input\n",
        "    with torch.no_grad():\n",
        "      for i,l in enumerate(self.layers):\n",
        "        x = l.forward(x)\n",
        "      return x\n",
        "  \n",
        "  # pass correct_vals as a list of length 64, where each item is the correct value for a given index.\n",
        "  #Generates a heatmap showing the predictions for a batch of 64 images.\n",
        "  def heat_map(self, preds, epoch, correct_vals, name):\n",
        "    dframe = pd.DataFrame(preds.numpy())\n",
        "    dframe.index += 1\n",
        "    dframe = dframe.T\n",
        "    sns.set(rc={\"figure.figsize\":(32,4.5)})\n",
        "    ax = sns.heatmap(dframe, vmin = 0)\n",
        "    plt.xlabel(\"Images\")\n",
        "    plt.ylabel(\"Predictions for labels\")\n",
        "    plt.title(\"Predictions for a batch after \" + str(epoch+1) + \" Epochs with \" + name)\n",
        "    pred_vals = torch.argmax(preds[:], dim=1)\n",
        "    for num, val in enumerate(pred_vals):\n",
        "      ax.add_patch(Rectangle((num, val), 1,1, fill=False, edgecolor='cyan', lw=5))\n",
        "    for num, val in enumerate(correct_vals):\n",
        "      ax.add_patch(Rectangle((num, val), 1,1, fill=False, edgecolor='green', lw=3))\n",
        "    plt.savefig(f\"{img_dir}/{name}_{self.dataset}_{self.fn}_{epoch}_Heatmap.png\")\n",
        "    plt.show()\n",
        "\n",
        "  #Outputs the accuracy of the model for the test data given.\n",
        "  def test_accuracy(self, test):\n",
        "    accs = []\n",
        "    for i,(input, label) in enumerate(test):\n",
        "      pred = self.forward_pass(input.to(device))\n",
        "      #print(\"Input: \", input)\n",
        "      #print(\"Label: \", label)\n",
        "      #print(\"Predicted: \", pred)\n",
        "      accs.append(accuracy(pred, onehot(label, layers[-1].output_size).to((device))))\n",
        "    return np.mean(np.array(accs)), pred\n",
        "    \n",
        "  #Updates the fully connected weights matrix\n",
        "  def update_error_weights(self):\n",
        "    for i in range(1,self.l_num):\n",
        "      d_error_connection = torch.matmul(self.guess[i].T,self.pred_e[i])\n",
        "      self.error_weights[i] = self.error_weights[i].clone() - (self.weight_learning_rate * torch.clamp(d_error_connection*2,-1000, 1000))\n",
        "\n",
        "  #Updates the weights on each layer\n",
        "  def update_weights(self):\n",
        "    for (i, l) in enumerate(self.layers):\n",
        "      l.update_weights(self.pred_e[i+1])\n",
        "\n",
        "  #Performs inference for the value neurons, updating the predictions every step.\n",
        "  def equalise_net(self):\n",
        "    #Initial error setup\n",
        "    for i in range(1, self.l_num+1): #Not including input layer\n",
        "      self.guess[i] = self.layers[i-1].forward(self.mus[i-1]) #Guess for current layer from layer below\n",
        "      if self.fully_connected:\n",
        "        self.pred_e[i] = self.mus[i] - (self.guess[i] @ self.error_weights[i]) #Calculates the error between current layer mu and guess * connection weights\n",
        "      else:\n",
        "        self.pred_e[i] = self.mus[i] - self.guess[i]\n",
        "    for n in range(self.n_steps):\n",
        "      for layer in range(1, self.l_num):\n",
        "        self.down_error[layer] = self.layers[layer].backward(self.pred_e[layer+1])\n",
        "        #Difference between layers\n",
        "        d_l = self.pred_e[layer] - self.down_error[layer]\n",
        "        self.mus[layer] -= self.inf_learning_rate * torch.clamp(d_l*2,-1000, 1000)\n",
        "      for i in range(1, self.l_num+1): #Not including input layer\n",
        "        if self.fully_connected:\n",
        "          self.pred_e[i] = self.mus[i] - (self.guess[i] @ self.error_weights[i]) #Calculates the error between current layer mu and guess * connection weights\n",
        "        else:\n",
        "          self.pred_e[i] = self.mus[i] - self.guess[i]\n",
        "\n",
        "  #Trains the network on a given batch.\n",
        "  def learn(self, input, label, epoch):\n",
        "    with torch.no_grad():\n",
        "      #Initialise the input to the network as the image\n",
        "      self.mus[0] = input.clone() \n",
        "      #Pass the mus up the layers to initialise\n",
        "      for i in range(1,self.l_num):\n",
        "        self.mus[i] = self.layers[i-1].forward(self.mus[i-1])\n",
        "      #Set the top layer equal to the correct outputs\n",
        "      self.mus[-1] = label.clone()\n",
        "      self.equalise_net()\n",
        "      self.update_weights() \n",
        "      #Update the weights after each batch\n",
        "      if self.fully_connected:\n",
        "        self.update_error_weights()\n",
        "\n",
        "  #Trains the network on the given dataset that has been organised into batches\n",
        "  def train_net(self, training, testing, test_name, n_epochs=1):\n",
        "    with torch.no_grad():\n",
        "      test_accs = []\n",
        "      for e in range(n_epochs):\n",
        "        self.weight_learning_rate = self.weight_learning_rate*0.75\n",
        "        for l in self.layers:\n",
        "          l.learning_rate = l.learning_rate*0.75\n",
        "        print(e)\n",
        "        for i,(inp, label) in enumerate(training): #Trains the model on each batcn\n",
        "          #print(\"Batch: \", i)\n",
        "          self.learn(inp.to(device), onehot(label, layers[-1].output_size).to(device), e)\n",
        "        test_mean, predictions_hm = self.test_accuracy(testing) #Test the accuracy of the model using the training data after each epoch\n",
        "        test_accs.append(test_mean)\n",
        "        if e%5 == 0:\n",
        "            self.heat_map(predictions_hm.to(\"cpu\"), e, testing[-1][1], test_name) #Generates heatmap of the last batch of the test data every 5 epochs\n",
        "      self.heat_map(predictions_hm.to(\"cpu\"), e, testing[-1][1], test_name) #Generates heatmap at the end of the training.\n",
        "      return test_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEmyLquyFpDZ"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs all tests for every variable version of the predictive coding model on all 3 datasets and each of the 3 activation functions we decided to use."
      ],
      "metadata": {
        "id": "FSILQ6YILbLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8msYrzt2FsMp"
      },
      "outputs": [],
      "source": [
        "functions = [(tanh, d_tanh, \"Tanh\"), (relu, d_relu, \"reLU\") , (sigm, d_sigm, \"Sigmoid\")]   \n",
        "datasets = [\"cifar\", \"fashion\",\"mnist\"]\n",
        "#Format: (Seperate backward weights, Backward non-linearitys, Full connections)\n",
        "variables = [(False, True, False, \"Original Predictive Network\"), (False, False, False, \"No backward non-linearitys\"), (True, True, False, \"Seperate Backward Weights\"), (False, True, True, \"Fully connected layers\"), (True, False, True, \"Combined Relaxed Network\")]  \n",
        "output_size = 10\n",
        "num_epochs = 30\n",
        "origin_pred = []\n",
        "for v in variables:\n",
        "  for d in datasets:\n",
        "    if d == \"cifar\":\n",
        "        input_size = 3072\n",
        "    elif d == \"fashion\" or d == \"mnist\":\n",
        "        input_size = 784\n",
        "    else:\n",
        "      print(\"Not a valid dataset!\")\n",
        "      break\n",
        "    training, testing, display = import_data(d)\n",
        "    for (f, d_f, name) in functions:\n",
        "      #Initialise layers\n",
        "      l1 = Pred_Layer(input_size, 500, activation_function=f, act_func_deriv=d_f, use_back_weights = v[0], use_non_linear = v[1])\n",
        "      l2 = Pred_Layer(500, 200, activation_function=f, act_func_deriv=d_f, use_back_weights = v[0], use_non_linear = v[1])\n",
        "      l3 = Pred_Layer(200, 100, activation_function=f, act_func_deriv=d_f, use_back_weights = v[0], use_non_linear = v[1])\n",
        "      l4 = Pred_Layer(100, output_size, activation_function=f, act_func_deriv=d_f, use_back_weights = v[0], use_non_linear = v[1])\n",
        "      layers = [l1,l2,l3,l4]\n",
        "      net = Relaxed_Predictive_Net(layers, name, d, fully_connected = v[2])\n",
        "      results = net.train_net(training, testing, v[3], num_epochs)\n",
        "    if v[3] == \"Original Predictive Network\":\n",
        "      origin_pred = pd.DataFrame(results)\n",
        "    else:\n",
        "      depict(results, name, d, v[3], origin_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Individual Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}